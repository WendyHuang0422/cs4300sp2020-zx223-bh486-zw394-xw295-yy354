# -*- coding: utf-8 -*-
"""AdHoc_Data_Crawl.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xFcSLlnUHSAFbXIwlPbkiYkU80J-gtgP

#Readme
for front end users:

the only funciton you need to connect to is

totally_aggregated(celeb_name,N_tweets,ad_hoc,N_keyword = 5,num_processed_tweets=200,num_pool_tweets=300,nltk=False)


call this funciton\
specifications on how to use this function are added in the documentation of the funciton\
\
currently only supports ad_hoc mode,
\
\
before calling this funciton\
you need to handel the library dependency\
which includes the following commands:
\
\
import tweepy\
import re\
import datetime\
import numpy as np\
import urllib\
import json\
import math\
import nltk\
nltk.download('stopwords')\
import pymongo\
from pymongo import MongoClient\
import datetime\
from datetime import datetime, timedelta\
pip install newspaper3k\
pip install vaderSentiment
from newspaper import Article\
\
today's rate limit has been reached, please just write your\
website assuming that the return you receive totally_aggregated\
is in the following format\

(1) the list of dictionaries with each dictionary representing a tweet\
    it is in the folowing format:
    [{'text':,'created_at':,'id':,'retweet_count':,'favorite_count':,'url'},...]
\
\
\
(2) a dictionary,key is a index in (1) and value is a list of index from (3)\
    for example, {1:[1,2,3],0:[16,20,70],89:[1213,234,657]}
    1:[1,2,3] means the related news of the tweet at index 1 in (1)
    are 1,2,3 in (3)

(3)a list of dictionaries,each dictionary represents a piece of news\
  [{'source':,'author':,'description':,'publi_time':,'url':,'content':},...]\
  or\
  [{'source':,'author':,'description':,'publi_time':,'url':,'content':},...]
"""

from dateutil.parser import parse
from datetime import datetime
import itertools
import time
from IPython.core.display import HTML
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import vaderSentiment.vaderSentiment
from collections import defaultdict
from nltk.corpus import wordnet as wn
from nltk.stem import WordNetLemmatizer
import tweepy
import re
import datetime
import numpy as np
import urllib
import json
import math
import nltk
# nltk.download('stopwords')
import pymongo
from pymongo import MongoClient
import datetime
from datetime import timedelta
# !pip install newspaper3k
from newspaper import Article
from nltk.tokenize import RegexpTokenizer
import nltk
# nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
nltk.download('wordnet')


def create_ApI(consumer_key, s_consmer_key, access_key, s_acess_key):
    # given the authroization return a tweepy api object for retrieving data

    auth = tweepy.OAuthHandler(consumer_key, s_consmer_key)
    auth.set_access_token(access_key, s_acess_key)
    api = tweepy.API(auth)
    return api


def tweet_score(status, timeline):
    """
    the current version assign score simply by the time-stamp
    the greater the score assigned is, the more relevant it is
    status: the particular status that we want to assign a score to
    timeline: the entire list of retrieved status
    """

    time = [x for x in list(
        filter(None, re.split('[\s:-]', str(status.created_at))))]

    score = datetime.datetime.timestamp(
        datetime.datetime.strptime(' '.join(time), '%Y %m %d %H %M %S'))

    temp = status._json
    if 'full_text' in temp.keys():
        text = temp["full_text"]
    else:
        text = temp["text"]
    if len(text.split()) < 20:
        score = score-10000000000
    return score


def complete_replies(base, status, fetched, api):
    if status._json['in_reply_to_status_id'] == None:
        return
    else:
        responded = api.get_status(
            id=status._json['in_reply_to_status_id'], tweet_mode='extended')
        base._json['full_text'] = responded._json['full_text'] + \
            base._json['full_text']
        fetched.add(responded._json['id'])
        complete_replies(base, responded, fetched, api)


def retrieve_tweets(N, celeb_name, inc_retweets, api):
    """
    help to retrieve the needed number of tweets beyond the number 20
    this function does not assigne scores to each tweets
    N: the number of wanted tweets
    celeb_name: the screen name of the celebrity
    inc_retweets: a boolean varibale indicating whether retweets
                  are going to be included
    """
    if inc_retweets:
        return list(api.user_timeline(screen_name=celeb_name, count=N, tweet_mode='extended'))
    print(N)
    pool = list(tweepy.Cursor(api.user_timeline, screen_name=celeb_name,
                              tweet_mode='extended', count=200).items())
    print(pool)
    result = []
    count = 0
    fetched = set()
    test = 0

    print(len(tweepy.Cursor(api.user_timeline, screen_name=celeb_name,
                            count=200, tweet_mode='extended').pages()))

    for page in tweepy.Cursor(api.user_timeline, screen_name=celeb_name, count=200, tweet_mode='extended').pages():
        for status in page:
            if status._json['id'] not in fetched:
                complete_replies(status, status, fetched, api)
            if not inc_retweets:
                if 'retweeted_status' not in status._json.keys():
                    result.append(status)
            else:
                result.append(status)
            if len(result) >= N:
                return result[:N]

    return (None, 'the user made no recent tweets, try include retweets')


def tweets_crawl(N, pool_size, celeb_name, api, inc_retweets):
    """
      retrieve the top N tweets tweeted by the celeb_account
      N: the number of wanted accounts

      celeb_name: a string, the screen_name of the celebrity
      api: the tweepy api object
      pool_size: the size of the pool of potential tweets from which we retrieve
        N most significant
      inc_retweets: whether we include retweets, set to False to retrieve only
      original tweets
    """
    timeline = retrieve_tweets(pool_size, celeb_name, inc_retweets, api)
    if timeline[0] is None:
        # print(timeline[1])
        print("enter")
        return
    sorted_timeline = []
    time_stamp = {}
    for status_id in range(len(timeline)):
        status = timeline[status_id]
        score = tweet_score(status, timeline)
        news_score = (status, score)
        sorted_timeline.append(news_score)
        time_stamp[status._json['id']] = status_id

    index_time = {}
    sorted_timeline = sorted(sorted_timeline, key=lambda x: x[1], reverse=True)
    for tup_idx in range(len(sorted_timeline[:N])):
        index_time[tup_idx] = time_stamp[sorted_timeline[tup_idx][0]._json['id']]
    return sorted_timeline[:N], index_time


def twittter_aggregated(N, pool_size, celeb_name, inc_retweets):
    """
    the aggregated retrieval function  to retrieve N most recent tweets from pool_size
    potential tweets, for example, 200 tweets from 400 tweets in the format
    specified in the google doc (a list of dictionaries with each dictionary
    representing a tweet).
    Also returned is a dictionary mapping the index in the list to the time_stamp

    Call this function in the other stages of the system

    READ: this function depends on
    tweepy
    math
    urllib
    json
    import these modules before calling

    N: the number of wnated accounts

    celeb_name: a string, the screen_name of the celebrity
    api: the tweepy api object
    pool_size: the size of the pool of potential tweets from which we retrieve
      N most significant
    inc_retweets: whether we include retweets, set to False to retrieve only
      original tweets
    """

    c_key = '4xQ7FcDfGAlAM5JkG505ndS3k'
    s_c_key = 'GJkKVtF34AoqaXSdCXq6agxCKrj1T2FL9i2w28Yj9t2wCo6jmM'
    a_key = '1247186585994637312-6ZuAFM9bhsv1Mil7utLrL0stxbT8ft'
    s_a_key = 'PkPrIdOOPSoQwCM0TuKF4dqLiKOOdTemEpjh2Ewf44FUd'
    api = create_ApI(c_key, s_c_key, a_key, s_a_key)
    user = api.get_user(screen_name='realDonaldTrump')
    data = []

    wanted = ['text', "id_str", 'created_at', 'id',
              'retweet_count', 'favorite_count', "user"]
    # xzh_wanted_keys = ["user"]
    # xzh_wanted_user_keys = ["followers_count", "friends_count", "listed_count", "favourites_count", "statuses_count", \
    # "profile_banner_url", "profile_image_url_https"]
    count = 0
    recent, idx_time = tweets_crawl(
        N, pool_size, celeb_name, api, inc_retweets)
    for idx in recent:
        idx = idx[0]._json
        temp = {}

        if 'full_text' in idx.keys():
            temp['text'] = idx['full_text']
        else:
            temp['text'] = idx['text']
        for x in wanted[1:]:
            temp[x] = idx[x]
        sid_obj = SentimentIntensityAnalyzer()
        sentiment_dict = sid_obj.polarity_scores(temp['text'])
        rank = [sentiment_dict['pos'],
                sentiment_dict['neu'], sentiment_dict['neg']]
        temp['sentiment'] = ['positive', "neutral",
                             'negative'][rank.index(max(rank))]
        data.append(temp)
    return data, idx_time


# https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object
# all these keys should go into a json (or the provided secrect environment file
# later for secrecy


# a = twittter_aggregated(20,30,'realDonaldTrump',False)


# with open('/content/drive/My Drive/Twitter_Account.txt',"w") as outfile:
    # json.dump(data,outfile)

"""News API"""


def retrieve_tweets(N, celeb_name, inc_retweets, api):
    """
    help to retrieve the needed number of tweets beyond the number 20
    this function does not assigne scores to each tweets
    N: the number of wanted tweets
    celeb_name: the screen name of the celebrity
    inc_retweets: a boolean varibale indicating whether retweets
                  are going to be included
    """
    if inc_retweets:
        return list(api.user_timeline(screen_name=celeb_name, count=N, tweet_mode='extended'))
    print(N)
    pool = list(tweepy.Cursor(api.user_timeline, screen_name=celeb_name,
                              tweet_mode='extended', count=200).items())
    result = []
    count = 0
    fetched = set()
    test = 0
    for page in tweepy.Cursor(api.user_timeline, screen_name=celeb_name, count=200, tweet_mode='extended').pages():
        for status in page:
            if status._json['id'] not in fetched:
                complete_replies(status, status, fetched, api)
            if not inc_retweets:
                if 'retweeted_status' not in status._json.keys():
                    result.append(status)
            else:
                result.append(status)
            if len(result) >= N:
                return result[:N]

    return (None, 'the user made no recent tweets, try include retweets')


def raw_news_retrieval(query, api_key, date1, date2, N, page, sort):
    """
    return a json file specified in
    query: a tuple or a keyword, the tuple should represent

    api_key: the auth key to retrieve the news
    time_span: a string in the format of 'yyyy-mm-dd:yyyy-mm-dd' indicating the
              timespan from the first date to the second one
    """
    # build the url
    sources = "abc-news,cbs-news,associated-press,bloomberg,nbc-news,fox-news,reuters,usa-today,business-insider,the-hill,espn,axios,bbc-news"
    keys_a = "&apiKey=" + api_key
    date_a = '&to='+date2+'&from='+date1
    if not query is None:
        query = 'everything?q='+query + '&'
    else:
        query = 'everything?q='
    url = "https://newsapi.org/v2/"+query\
          + '/page='+str(page)+'&pageSize=' + str(N) + \
        '&sortBy=' + sort + date_a+"&language=en"+"&sources="+sources+keys_a

    agg_file = json.load(urllib.request.urlopen(url))

    return agg_file


def retrieve_news_article(N, key, date1, date2, order, query):
    """
    retrieve N top results from the news API pool of news report

    N: the total number of wanted news
    key: the API key for access
    query: the string query terms for the search, can be in the format
     of  "keyword", "keyword AND keyword", "keyword NOT keyword",
     "keyword OR keyword"
     query (any operator) (query),
     keyword (any operator) query
     default is None
    date11: the start date of the timespan from which we retrieve news
      format: "yyyy-mm-dd"
    date2: the end date of the time span from which we retrieve news
    order: one of 'pubishedAt', 'relevancy',or 'popularity'
    """
    results = []
    date = date1+':'+date2
    results_left = 1
    page = 0
    while results_left > 0:
        instance = raw_news_retrieval(
            query, key, date1, date2, 100, page, 'publishedAt')
        totalSize = instance['totalResults']
        results.extend(instance['articles'])
        results_left = totalSize-len(results)
        if len(results) >= N:
            return results[:N]
        page = page + 1
    #print("there are not enoguh results that can be retrieved, returning as many as we can")
    return (results)


def news_Aggregated(N, date1, date2, order, query=None):
    """
    retrieve N top results from the news API pool of news report
    in the format specified in the degin doc (list of news, each represented by
    a dicitonary)

    N: the total number of wanted news
    query: the string query terms for the search, can be in the format
     of  "keyword", "keyword AND keyword", "keyword NOT keyword",
     "keyword OR keyword"
     query (any operator) (query),
     keyword (any operator) query
     default is None
    date11: the start date of the timespan from which we retrieve news
      format: "yyyy-mm-dd"
    date2: the end date of the time span from which we retrieve news

    (the time span can at most be one month)

    order: one of 'pubishedAt', 'relevancy',or 'popularity'

    libraries:
    re
    urllib
    json
    """
    keyss = ['2036183222ab402cbce56ee293728fb1',
             '8ba8b6889b1343508e1ba0b55849ec31']
    instance = retrieve_news_article(N, keyss[0], date1, date2, order, query)
    wanted = ['source', 'author', 'description',
              'publi_time', 'url', 'content']
    full1 = []
    count = 0
    for idx in instance:
        data1 = {x: None for x in wanted}
        data1['source'] = idx['source']['name']
        data1['author'] = idx['author']
        data1['description'] = idx['description']
        data1['publi_time'] = idx['publishedAt']
        data1['url'] = idx['url']
        data1['content'] = idx['content']
        full1.append(data1)
        count += 1
    return full1


def bing_retrieve_raw(N, query, key):
    """
    retrieve API results from Bing News API specified under
    https://docs.microsoft.com/en-us/rest/api/cognitiveservices-bingsearch/bing-news-api-v7-reference

    N: the number of results wanted
    query: the search keywords

    """
    # Bing API
    search_url = "https://api.cognitive.microsoft.com/bing/v7.0/news/search"
    headers = {"Ocp-Apim-Subscription-Key": key}
    q = ''
    if not query is None:
        q = query
    params = {"q": q, 'count': 100, 'offset': 1, 'mkt': 'en-US'}
    response = requests.get(search_url, headers=headers, params=params)
    response.raise_for_status()
    search_results = response.json()

    # while collected != N:
    return search_results


"""Codes for Reddit API, now abandoned"""

# def reddit_sub_retrieve(N,query,date1,date2):
#   """
#   https://pushshift.io/api-parameters/
#   thanks pushshift
#   """
#   url = "https://api.pushshift.io/reddit/submission/search/?"
#   query_t = ''
#   if query != None:
#     query_t = 'q=' + query + '&'
#   date1_e = 'after'+ str(datetime.strptime(date1,'%Y-%m-%d-%H-%S').timestamp())
#   date2_e = '&before'+ str(datetime.strptime(date2,'%Y-%m-%d-%H-%S').timestamp())
#   url = url + query_t + date1_e + date2_e + '&subreddit=news'
#   agg_file = json.load(urllib.request.urlopen(url))
#   return agg_file

# instance = reddit_sub_retrieve(0,None,'2020-04-09-11-05','2020-04-10-12-05')

"""Guardian"""


def generate_q(query):
    result = ''
    if type(query) == str:
        # the base case
        return query
    else:
        if query[1] == 0:
            op = '%20NOT%20'
            return '('+op + generate_q(query[0])+')'
        else:
            if query[1] == 1:
                op = '%20AND%20'
            else:
                op = '%20OR%20'
            return '('+generate_q(query[0][0]) + op + generate_q(query[0][1])+')'


def raw_g_retrieval(query, api_key, time_span, N, page):
    """
    return a json file specified in https://open-platform.theguardian.com/documentation/search
    query: a tuple or a keyword, the tuple should represent
          a relation
          ((a,b),1) -> a and b
          ((a,b),2) -> a or b
          ((a,(b,0)),1) -> a and NOT b
          (((a,b),1),(((c,d),2),0),1) = (a AND B) AND NOT (c OR d)
          means reddit AND NOT news AND (random or guardian)
    api_key: the auth key to retrieve the news
    time_span: a string in the format of 'yyyy-mm-dd:yyyy-mm-dd' indicating the
              timespan from the first date to the second one
    """
    # build the url
    date = re.split(':', time_span)
    keys_a = "&api-key=" + api_key
    date_a = '&to-date='+date[1]+'&from-date='+date[0]
    if not query is None:
        query = 'search?q='+query + '&'
    else:
        query = 'search?'
    url = "https://content.guardianapis.com/"+query+'show-blocks=all'+'&format=json'\
          + '&page='+str(page)+'&page-size=' + str(N)+date_a+keys_a

    print(url)
    agg_file = json.load(urllib.request.urlopen(url))
    return agg_file


def retrieve_g_article(N, key, query, date1, date2):
    results = []
    date_counter = date2+':'+date2
    start_date = datetime.strptime(date2, '%Y-%m-%d')
    end_date = datetime.strptime(date1, '%Y-%m-%d')
    while len(results) < N and start_date >= end_date:
        instance = raw_g_retrieval(query, key, date_counter, 200)
        for idx in instance['response']['results']:
            if idx['type'] == 'article':
                results.append(idx)
                if len(results) == N:
                    break
        start_date = start_date - timedelta(days=1)
        new_date = start_date.strftime("%Y-%m-%d")
        date_counter = new_date+':'+new_date
    return results


def retrieve_g_article1(N, key, query, date1, date2):
    results = []
    date = date1+':'+date2
    page_left = 1
    page = 1
    while page_left != 0:
        instance = raw_g_retrieval(query, key, date, 200, page)

        pageSize = instance['response']['pageSize']
        if pageSize < page:
            return results
        currentPage = instance['response']['currentPage']
        for idx in instance['response']['results']:
            if idx['type'] == 'article':
                results.append(idx)
                if len(results) >= N:
                    return results

        page_left = instance['response']['pages'] - \
            instance['response']["currentPage"]
        page = page + 1
    return results


def guardian_aggregated(N, query, date1, date2):
    key = "7dd62f93-de59-4cb6-a166-e7c44543477d"
    raw_jsons = retrieve_g_article1(N, key, query, date1, date2)
    wanted = ['webPublicationDate', 'webTitle', 'webUrl', ]
    order = ['blocks']
    result = []
    for instance in raw_jsons:
        temp = {}
        for key in wanted:
            temp[key] = instance[key]
        if(len(instance['blocks']['body'][0]['bodyTextSummary']) > 0):
            temp['text'] = instance['blocks']['body'][0]['bodyTextSummary']
        else:
            temp['text'] = instance['blocks']['body'][0]['bodyHtml']

        result.append(temp)
    return result


# print(guardian_aggregated(1, None,'2020-04-14','2020-04-16')[0]['webPublicationDate'])

"""# Representation And Mathcing"""

# 1.step in search engine, after the user types the celevrity_name and topic

# this function CONNECTS TO FRONT END


def retrive_twitt_post_url(top_num, N, celebrity_name, topics):
    """
    [top_num]: most recent [top_num] twitts by the [celebrity_name]
    [N]: choosing from N recent posts
    [topics]: string of relating topics ; "" if no specified the topics

    Return: A list of tuples: ( twitter post content, corresponding url)
            the list may have (length < top_num)
            if the celebrity doesn't have [top_num] of post related to the topic
    """
    post_url = []
    twitts, idx_time = twittter_aggregated(N, N, celebrity_name, False)
    post_url = decompose_post(twitts)
    if topics == "":
        return post_url[:top_num]
    else:
        flat_twitts = [m[0] for m in post_url]
        twitts_inverted_ind = build_inverted_ind(flat_twitts)
        top_related = find_related_post(
            top_num, topics, post_url, twitts_inverted_ind)
        return top_related

# Helper functions for 1.step:


def decompose_post(twitts):
    """
    Decompose the twitts to seperate the url and the text from the twitts content
    """
    post_url = []
    for twitt in twitts:
        text = twitt['text']
        urls = re.findall(r'(https?://\S+)', text)
        for url in urls:
            text = text.strip(url)
        if len(urls) > 0:
            post_url.append((text, urls[-1]))
        else:
            post_url.append((text, None))
    return post_url


def tokenize_tweets(tweetList_raw, stemming=False, pos=False, lower=True, remove_stop=True, nltk1=True):
    """
    tokenize the tweets decompsed into acutal texts and urls
    return a list of tokenized tweets and urls

    tweetList_raw: the original list of dictionaries
    stemming: if we only put the word stem into the processed tweets
    pos: if we only preserve Noun phrases
    lower : whether we turn all the words into lower cases. default is true
    parse: using nltk or spacy, nltk is way faster, default is nltk

    note: stemming and pos cannot both be set to true, if both are set to
    true default to only using POS

    """
    if stemming and pos:
        stemming = False
    result = []
    flattened = decompose_post(tweetList_raw)
    stemmer = nltk.PorterStemmer()
    tokenizer = nltk.RegexpTokenizer(r"\w+")
    parse = None
    if not nltk1:
        print('we are here')
        parser = spacy.load('en_core_web_sm')

    wanted = ['N', 'P', 'J', 'V', 'P']  # 'VB'
    wanted_scp = ['ADJ', 'NOUN', 'VERB', 'PROPN', 'PROPN']
    for tweet in flattened:
        sentences = nltk.sent_tokenize(tweet[0])
        tokens = []
        for sent in sentences:
            temp = sent
            rich_word = None
            if lower:
                temp = sent.lower()
            if nltk1:
                sent_tok = nltk.word_tokenize(temp)
            if not nltk1:
                rich_word = parser(temp)
                sent_tok = [x.text for x in rich_word]
            if pos:
                if nltk1:
                    tags = [x[1] for x in nltk.pos_tag(sent_tok)]
                    sent_tok = [sent_tok[i] for i in range(
                        len(sent_tok)) if tags[i][:1] in wanted]
                elif not nltk1:
                    tags = [word.pos_ for word in rich_word]
                    sent_tok = [sent_tok[i] for i in range(
                        len(sent_tok)) if tags[i] in wanted_scp]
            elif stemming:
                for idx in range(len(sent_tok)):
                    sent_tok[idx] = stemmer.stem(sent_tok[idx])

            if remove_stop:
                stop_words = stopwords.words('english')
                temp = []
                for tok in sent_tok:
                    if tok not in stop_words:
                        temp.append(tok)
                sent_tok = temp

            tokens.extend(sent_tok)
        result.append((tokens, tweet[1]))
    return result, tweetList_raw


def build_inverted_idx_zw(tokenized_tweets):
    """
    build inverted index for the list of twitter posts content
    tokenized_tweets: list of tokenized tweets
    return : a dictionary:  {term1: (doc_id, count_of_term1_in_doc),
                            term2: (doc_id, count_of_term2_in_doc) ... }
    """
    result = defaultdict(lambda: [], {})
    for i in range(len(tokenized_tweets)):
        tweet = tokenized_tweets[i][0]
        sofar = set()
        for tok in tweet:
            if(tok not in sofar):
                result[tok].append((i, tweet.count(tok)))
                sofar.add(tok)
    return result


def compute_idf(inv_idx, n_docs, min_df=1, max_df_ratio=1):
    result = {}
    for word in inv_idx:
        num = len(inv_idx[word])
        if(num >= min_df and num <= max_df_ratio*n_docs):
            result[word] = math.log(n_docs/(1+num), 2)
    return result


def compute_doc_norms(index, idf, n_docs):
    result = [0]*n_docs
    for word in index:
        per = index[word]
        for ind in per:
            if(word in idf):
                result[ind[0]] += (ind[1]*idf[word])**2
    norm = np.sqrt(result)
    return norm


def build_inverted_ind(docList):
    """
    build inverted index for the list of twitter posts content
    tweetList_raw: the original list of dictionaries
    return : 1.a dictionary:  {term1: (doc_id, count_of_term1_in_doc),
                            term2: (doc_id, count_of_term2_in_doc) ... }
             2.the tokenized sentences_url pairs
             3.
    """
    tokenizer = RegexpTokenizer(
        r'((?<=[^\w\s])\w(?=[^\w\s])|(\W))+', gaps=True)
    result = defaultdict(lambda: [], {})
    for i in range(len(docList)):
        toks = tokenizer.tokenize(docList[i])
        sofar = set()
        for tok in toks:
            if(tok not in sofar):
                result[tok].append((i, toks.count(tok)))
                sofar.add(tok)
    return result


"""# Experimenting Clustering By Topic"""


# print(len(examples))
# for ex in examples:
#   print(ex['created_at'])
#   print(len(ex['text'].split()))
#   print('\n')

# !python -m spacy.en.download all

# experimenting clustering by topics
nltk.download('stopwords')
#import spacy
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')


def idf_topic_cluster(inv_idx, tweet1, tok_tweet_list, idf, doc_norms):

    scores = []    # sorted list of similarities
    query_tok = tok_tweet_list[tweet1][0]
    query_norm, scores = doc_norms[tweet1], [0]*len(tok_tweet_list)
    temp = defaultdict(lambda: 0, {})

    # idf = []
    for tok in query_tok:
        if(tok in idf):
            temp[tok] += 1
    for tok in temp:
        tweets = inv_idx[tok]
        for tweet in tweets:
            scores[tweet[0]] += (idf[tok])**2*temp[tok]
    inv_idx = {key: val for key, val in inv_idx.items() if key in idf}
    doc_norms = np.multiply(math.sqrt(query_norm), doc_norms)
    orders = []
    for i in range(len(doc_norms)):
        if(doc_norms[i] != 0 and scores[i]):
            orders.append((1.0*scores[i]/doc_norms[i], i))
    # print(orders)
    scores = sorted(orders, key=lambda x: x[0], reverse=True)
    top = scores[0][0]
    result = [(tok_tweet_list[record[1]], record[1])
              for record in scores if record[0] > 1/4*top]

    return result


def recent_surge(word, tweet, total_length, range_per, tok_tweet_list, inv_idx):
    """
    segments: a percentage indicating whih segment around the current tweet do we examine
    """
    segment = total_length*range_per
    covered_length = 0
    pre = tweet
    pos = tweet  # moving boundaries
    last = 0 if tweet > 0 else 1
    count = 0
    while covered_length < segment:

        if last == 0 and pre-1 >= 0:
            pre = pre - 1
            covered_length += len(tok_tweet_list[pre][0])
            if pos < len(tok_tweet_list):
                last = 1
        elif last == 1 and pos < len(tok_tweet_list):
            covered_length += len(tok_tweet_list[pos][0])
            pos = pos + 1
            if pre-1 > -1:
                last = 0
    global_wd_count = sum([x[1] for x in inv_idx[word]])
    local_wd_count = 0
    for pair in inv_idx[word]:
        if pair[0] <= pos and pair[0] >= pre:
            local_wd_count += pair[1]

    global_per = global_wd_count/total_length
    local_per = local_wd_count/covered_length

    return math.log((local_per/global_per))


def keywords(tweet, pool, idf, tok_tweet_list, inv_idx):
    total_length = sum([len(x[0]) for x in tok_tweet_list])
    score = {}
    pool_text = []

    for text in pool:
        pool_text = text[0][0]

        for word in pool_text:
            if word in idf.keys():
                surge = recent_surge(
                    word, text[1], total_length, 0.1, tok_tweet_list, inv_idx)

                score[word] = score.get(word, 0) + idf[word]*surge
    score = sorted(list(score.items()), key=lambda x: x[1], reverse=True)
    return [x[0] for x in score]


def q_generate(combo, news_source):
    """
    basically a function to and the query terms together

    combo: the list of query terms
    """
    connect_tok = '+'
    if news_source == 'Guardian':
        connect_tok = '%20AND%20'
    return connect_tok.join(combo)


def retrive_news(list_keywords):
    """
    the function generates all combinations of the
    keyword set to retrieve the related news, if the longest combination
    have results then just retur that result, otherwise decrease the
    keywords by one until some news are retrieved.
    """
    result = []
    date1 = (datetime.datetime.today() -
             timedelta(days=30)).strftime("%Y-%m-%d")
    date2 = datetime.datetime.today().strftime("%Y-%m-%d")
    for length in range(len(list_keywords), int(0.4*len(list_keywords)), -1):

        combinations = itertools.combinations(list_keywords, length)
        query = ''
        count = 0
        for news_source in ['News', 'Guardian']:
            for combo in combinations:
                if count == 0:
                    query = '('+q_generate(combo, news_source)+')'
                else:
                    query += 'OR' + '('+q_generate(combo, news_source)+')'
                count += 1
            if news_source == 'Guardian':
                pass
                #result.extend(guardian_aggregated(10, query,date1,date2))
            else:
                result.extend(news_Aggregated(10, date1, date2,
                                              order='relevancy', query=query))
        if len(result) > 0:
            print(length, combo)
            return result
    return result


# this is a function that can be called by the front end
def keywords_per_tweet(list_of_tweets, idx_time, N_tweets, N_keyword, nltk1):
    """
    given a list of tweets for the first N_tweets tweets in the list
    for each one of them return a list of news deemed to be related to the
    tweet. In the format of:
    {tweet index in the list:[list of related news objects]}

    list_of_tweets: the list of tweets in the raw list of dictionaries
    idx_time: the dicitonary that map the index in the list of tweet to the
              original index in the list of tweets that strictly follows the chronological order
              for example, 101 to 50 means the 101th item in the list was originally the
              50th most recent tweet we retrieve
    N_tweets: the number of tweets for which we want to retrieve news
    N_keyword: the number of top keywords retrieved for each cluster of tweets

    """
    tokenized, tweetList_raw = tokenize_tweets(
        list_of_tweets, stemming=False, pos=True, lower=True, remove_stop=True, nltk1=nltk)
    tokenized_time_s = [None]*len(tokenized)
    print("tokenized")
    time_idx = {idx_time[x]: x for x in idx_time.keys()}

    for idx in range(len(tokenized)):
        tokenized_time_s[idx_time[idx]] = tokenized[idx]

    # the tokenization args can be adjusted for optimal performance
    # tweetList_raw the original list_of_tweets

    inv_idx = build_inverted_idx_zw(tokenized_time_s)
    idf = compute_idf(inv_idx, len(tokenized_time_s), max_df_ratio=0.7)
    doc_norms = compute_doc_norms(inv_idx, idf, len(tokenized_time_s))
    print("processed")
    result = {}
    for tweet_idx in range(N_tweets):
        tweet_in_time = idx_time[tweet_idx]
        print('converted')
        pool = idf_topic_cluster(
            inv_idx, tweet_in_time, tokenized_time_s, idf, doc_norms)
        print('topic clustered')
        keyword = keywords(tweet_in_time, pool, idf,
                           tokenized_time_s, inv_idx)[:N_keyword]
        print('got keywords', keyword)
        result[tweet_idx] = retrive_news(keyword)
        print('retrieved news')
    return result


def tweet_match_db(tok_tweet, inverted_idx, idf, doc_norms, list_of_news, doc_overlap):
    scores = []    # sorted list of similarities
    query_tok = tok_tweet
    query_norm, scores = 0, [0]*len(doc_norms.keys())
    temp = defaultdict(lambda: 0, {})
    # idf = []

    for tok in query_tok:
        if(tok in idf.keys()):
            temp[tok] += 1
    for tok in temp:
        query_norm += (temp[tok]*idf[tok])**2
        documents = inverted_idx[tok]
        for ind in documents:
            if ind[0] in doc_overlap.keys():
                scores[ind[0]] += ind[1]*(idf[tok])**2*temp[tok]
    maxium = max([len(x) for x in doc_overlap.values()])
    orders = []
    for i in range(len(doc_norms)):
        if i in doc_overlap.keys():
            over_score = len(doc_overlap[i])/maxium
            if(doc_norms[str(i)] != 0 and scores[i] != 0):
                sc = 1.0*scores[i]/doc_norms[str(i)]
                orders.append((sc+sc*over_score, i))
    # print(orders)
    scores = sorted(orders, key=lambda x: x[0], reverse=True)

    result = [(list_of_news[record[1]], doc_overlap[record[1]])
              for record in scores]
    # the results are tuples consisting of two parts, 1: the info list as before
    # 2: the number of covered keywords
    return result


def keywords_expansion(list_query):
    result = []
    for word in list_query:
        temp = [word]
        result.append(temp)
    return result


def find_overlap_doc(inverted_idx, key_words_cluster):
    doc_overlap = {}  # a dicitonary showing
    lemmatizer = WordNetLemmatizer()
    for cluster in key_words_cluster:
        temp = set()
        for key_word in cluster:
            l_o_key = lemmatizer.lemmatize(key_word)
            for keys in inverted_idx.keys():
                l_key = lemmatizer.lemmatize(keys)
                if l_o_key == l_key:
                    for doc in inverted_idx[keys]:
                        temp.add(doc[0])
        for doc1 in list(temp):
            doc_overlap[doc1] = doc_overlap.get(doc1, [])+[cluster[0]]
    return doc_overlap


def find_overlap_tweet(tokens, key_words_cluster):
    overlap_list = []
    idx_dict = {}
    lemmatizer = WordNetLemmatizer()
    for cluster in key_words_cluster:
        temp = set()
        for key_word in cluster:
            l_o_key = lemmatizer.lemmatize(key_word)
            for idx in range(len(tokens)):
                tok, _ = tokens[idx]
                for word in tok:
                    l_word = lemmatizer.lemmatize(word)
                    if l_o_key == l_word:
                        temp.add(idx)
        for doc1 in list(temp):
            idx_dict[doc1] = idx_dict.get(doc1, [])+[cluster[0]]
    for keys, value in idx_dict.items():
        overlap_list.append((tokens[keys], len(value), keys, value))
    return overlap_list


def db_news_retrieval(list_of_tweets, N_news, input_keys, N_tweets):
    # list of interested tweets dictionaries
    list_of_news = get_mongo_store('news')
    inverted_idx = get_mongo_store('inverted_index')
    doc_norms = get_mongo_store('document_norms')
    idf = get_mongo_store('idf')
    tokenized, tweetList_raw = tokenize_tweets(
        list_of_tweets, stemming=False, pos=True, lower=True, remove_stop=True, nltk1=True)

    if input_keys is not None:
        input_keys = re.split('\s*,\s*', input_keys.lower())
        key_words_cluster = keywords_expansion(input_keys)
        lemmatizer = WordNetLemmatizer()
        doc_overlap = find_overlap_doc(inverted_idx, key_words_cluster)

        tokenized = sorted(find_overlap_tweet(
            tokenized, key_words_cluster), key=lambda x: x[1], reverse=True)

        recog_doc = doc_overlap.keys()

        filtered_tweets = [(tweetList_raw[x[2]], x[3])
                           for x in tokenized][:N_tweets]

    else:
        recog_doc = [int(k) for k in doc_norms.keys()]
        tokenized = [(tokenized[x], 0, x, [None])
                     for x in range(len(tokenized))]
        doc_overlap = {x: [None] for x in recog_doc}
        filtered_tweets = [(tweetList_raw[x[2]], x[3])
                           for x in tokenized][:N_tweets]

    temp = {}
    for tweet_idx in range(len(tokenized[:N_tweets])):
        tweet, t_overlap, t_idx, _ = tokenized[tweet_idx]
        text = tweet[0]
        top_n = tweet_match_db(text, inverted_idx, idf,
                               doc_norms, list_of_news, doc_overlap)[:N_news]
        temp[tweet_idx] = []
        for result in top_n:
            info = result
            if tweet_idx in temp.keys():
                temp[tweet_idx].append(info)
            else:
                temp[tweet_idx] = [info]
    return temp, filtered_tweets


def totally_aggregated(celeb_name, N_tweets, ad_hoc, input_keys, N_keyword=5, num_processed_tweets=100, num_pool_tweets=200, nltk1=False):
    """
    the funciton should be called by the frontend as the ONLY connection point with the backend
    no other function, optimally, shoule be called by the frontend
    the function returns a dictionary, each one represents a news piece

    celeb_name: the screen name of the interested celebrrity
    N_tweets: the number of processed tweets that we attach related news pieces to
    ad_hoc: a boolean indicating whether we are doing ad hoc retrievl, or retrieving from
            our processed database
            if it is set to False, use online database
            if it is set to True, use News API to retrieve ad hoc

    the following arguments are used primarily to adjust the time performance of the
    code

    N_keyword: the number of keywords we extract from each tweets,the reccommended
               BE CAREFUL: currently the number is hard-coded to 5, changing the
               value has significant implications on the function's performance,
               the user is allowed to decrease the number but increasing it is not
               recommended.
    num_processed_tweets: the number of tweets we process into inverted index
    num_pool_tweets: the size of the pool of tweets that we retrieve from the twitter API
    nltk: whether we are using nltk parser or Scapy parser, note: using nltk parser will
          significantly improve our timw performance but the tokenized reuslts are not optimal.
    """
    retrieved_num = num_processed_tweets
    pool_size = num_pool_tweets
    if not ad_hoc:
        if input_keys is None:
            retrieved_num = N_tweets
            pool_size = N_tweets*2
        else:
            retrieved_num = 100
            pool_size = 100
    data, idx_time = twittter_aggregated(
        retrieved_num, pool_size, celeb_name, False)
    if ad_hoc:
        time_idx = {idx_time[x]: x for x in idx_time.keys()}
        fall_in_range = sorted(idx_time.values())

        for idx in range(len(fall_in_range)):
            idx_time[time_idx[fall_in_range[idx]]] = idx

    # rehashing the range of time stamp back to the same size of the retrieved results
    if ad_hoc:
        result = keywords_per_tweet(data, idx_time, N_tweets, N_keyword, nltk1)
        filtered_tweets = data[:N_tweets]
    else:
        result, filtered_tweets = db_news_retrieval(
            data, 10, input_keys, N_tweets)
    return filtered_tweets, result

# b=db_news_retrieval(examples[0],5)

# for items in


# a = totally_aggregated('JoeBiden',5,True,N_keyword = 6,num_processed_tweets=300,num_pool_tweets=400,nltk=False)
"""
example explaination:
retrieve 400 tweets from JoeBiden
use the top 300 tweets as the sample
for the top 5 tweets in the sample
calculate their N_keyword (in this case 6) keywords and use the keywords to
do ad hoc retrieval using News API
"""

# print(a[1][3])


# for_text,idx_time = twittter_aggregated(200,300,'realDonaldTrump',False)

# import datetime
# start = datetime.datetime.now()
# fall_in_range = sorted(idx_time.values())
# print(len(idx_time.values()))
# time_idx = {idx_time[x]:x for x in idx_time.keys()}
# for idx in range(len(fall_in_range)):
#   idx_time[time_idx[fall_in_range[idx]]] = idx

# print(max(idx_time.values()))
# k = keywords_per_tweet(for_text,idx_time,10,6)
# print(k)

# print(datetime.datetime.now() -start)

# for key in k.keys():
#   news = for_text[key]
#   print(news)
#   for news in k[key]:
#     print(news)
#   print('\n')

# import numpy as np
# Helper function: find the top_num relating to the topic twitter post


def find_related_post(top_num, topics, post_url, inv_idx):
    """
    [topics]: String of user input, the topics that user is interested in
    [twitts]: ([(content1, url1), (content2,url2), ...])
    Return: A list of the top_num in the N [twitts] that are related to the topic
    """
    scores = []    # sorted list of similarities
    query_tok = topics.split(" ")
    query_norm, scores = 0, [0]*len(post_url)
    temp = defaultdict(lambda: 0, {})
    idf = compute_idf(inv_idx, len(post_url))
    # idf = []
    for tok in query_tok:
        if(tok in idf):
            temp[tok] += 1
    for tok in temp:
        query_norm += (temp[tok]*idf[tok])**2
        sentences = inv_idx[tok]
        for ind in sentences:
            scores[ind[0]] += ind[1]*(idf[tok])**2*temp[tok]
    inv_idx = {key: val for key, val in inv_idx.items() if key in idf}
    doc_norms = compute_doc_norms(inv_idx, idf, len(post_url))
    doc_norms = np.multiply(math.sqrt(query_norm), doc_norms)
    orders = []
    for i in range(len(doc_norms)):
        if(doc_norms[i] != 0 and scores[i]):
            orders.append((1.0*scores[i]/doc_norms[i], i))
    # print(orders)
    scores = sorted(orders, key=lambda x: x[0], reverse=True)
    result = [post_url[record[1]] for record in scores]
    return result


# 2.step in search engine, after the user chooses the twitts, showing the related news
nltk.download('stopwords')


# this function CONNECT TO FRONT END
# it should only be called once in a day
def store_news(N, start_date):
    """
    [N]: total number of news stored in the database
    [start_date]: the starting date of the news to store
    return: A list of the news tuple
            [(news_content1, url1), (news_content2, url2), ...]
    """
    end_date = datetime.date(datetime.now()).strftime("%Y-%m-%d")
    news = news_Aggregated(N, start_date, end_date, 'popularity', None)
    return news

# tf-idf between tweets:
# add-on to the tf-idf matrix if two tweets similar with high overlap

# this function CONNECT TO FRONT END


def related_news(post_url, top_n):
    """
    [post_url]: the tuple of twitter post and its url (content1, url1)
    Return: A list of the [top_n] news most related to twitts
            a list of tuples [(news_content1, url1), (news_content2, url2), ...]
    """
    tokenizer = RegexpTokenizer(r'[a-zA-Z]{3,}')
    tokens = tokenizer.tokenize(post_url[0])
    stopw = stopwords.words('english')
    ps = PorterStemmer()
    tokens = [token for token in tokens if ps.stem(token) not in stopw]
    query = "+".join(tokens)
    end_date = datetime.date(datetime.now()).strftime("%Y-%m-%d")
    news = news_Aggregated(2, start_date, end_date, 'relevancy', query)
    result = []
    for new in news:
        result = (new['content'], new['url'])

    find_related_post(top_num, topics, post_url, inv_idx)
    return result


# simulate store 500 news in the backend

# news_stored = store_news(500, "2020-04-01")

# # retrive news related to twitt
# a = ("Sorry Fake News, it’s all on tape. I banned China long before people spoke up. Thank you @OANN",'https://t.co/OMxB0x7xC5')
# related_news(a, 2, "2020-04-01")

# news = news_Aggregated(2,"2020-04-01","2020-04-13",'relevancy',"virus")
# print(news)

"""## Extract data from daily storage

(currently support daily update on news api, inverted_index... to be continue)


1. input:   "news"  :  get the dictionary of the 2000 most recent aggregated news dicitonary
2. input:   "inverted_index"  :  get inverted index of the content of the full text in the news


1.   List item
2.   List item



(if you want to store more dictionary, feel free to add them to the file news.py in {daily_update} folder)
"""


"""
Please copy this function if you want to get the mongo store elements
"""


def get_mongo_store(source_name, frac=6):
    client = pymongo.MongoClient(
        'mongodb://alicia:alicia1@ds139019-a0.mlab.com:39019,ds139019-a1.mlab.com:39019/heroku_gczdn597?replicaSet=rs-ds139019')
    db = client.get_default_database()
    result = {}
    if source_name == "inverted_index":
        for i in range(frac):
            collection = db[source_name+str(i)]
            cursor = collection.find_one({})
            result.update(cursor['dictionary'])
    else:
        collection = db[source_name]
        cursor = collection.find_one({})
        result = cursor['dictionary']
    # print(result)

    client.close()
    return result


# a = get_mongo_store("news")
# print(len(a))

"""# Get news text from a list of url"""

# import nltk
# nltk.download('punkt')
# from newspaper import Article


def get_news_text(url_list):
    """
      Retreive the news text from the url listm return list of string in the given
      url input order
      (some url might not be avialble fo this method so the return list might be
      less than the given url link)
      [text1, text2, ...]

      url_list: a list of urls
      Return: a list of strings(news content)

      Required Library: newspaper3k, nltk
      """
    text_list = []
    for i in range(len(url_list)):
        url = url_list[i]
        article = Article(url)
        try:
            # this sometimes casue error due to bad http requests
            article.download()
            article.parse()
        except:
            continue
        text_list.append(article.text)
    return text_list


"""#Processing News and Saving News to DB"""


def full_text_integerate(list_dictionaries):
    """

    """
    urls = [news['url'] for news in list_dictionaries]
    full_text = get_news_text(url_list)
    tokenized = []
    for text_idx in range(len(full_text)):
        instance = list_dictionaries[text_idx]
        if full_text[text_idx] is None:
            if len(instance['description']) > len(instance['content']):
                full_text[text_idx] = instance['description']
            else:
                full_text[text_idx] = instance['content']
        del list_dictionaries[text_idx]['description']
        del list_dictionaries[text_idx]['content']
    tokenized = tokenize_news(
        full_text, stemming=False, pos=True, lower=True, remove_stop=True, nltk1=False)
    inverted_index = build_inverted_idx_zw(tokenized)

    return inverted_index, list_dicitonaries


def tokenize_news(news_texts, stemming=False, pos=False, lower=True, remove_stop=True, nltk1=True):
    if stemming and pos:
        stemming = False
    result = []
    stemmer = nltk.PorterStemmer()
    tokenizer = nltk.RegexpTokenizer(r"\w+")
    parse = None
    if not nltk1:
        parser = spacy.load('en_core_web_sm')
    wanted = ['N', 'P', 'J', 'V', 'P']  # 'VB'
    wanted_scp = ['ADJ', 'NOUN', 'VERB', 'PROPN', 'PROPN']
    for news in news_texts:
        sentences = nltk.sent_tokenize(news)
        tokens = []
        for sent in sentences:
            temp = sent
            rich_word = None
            if lower:
                temp = sent.lower()
            if nltk1:
                sent_tok = nltk.word_tokenize(temp)
            if not nltk1:
                rich_word = parser(temp)
                sent_tok = [x.text for x in rich_word]
            if pos:
                if nltk1:
                    tags = [x[1] for x in nltk.pos_tag(sent_tok)]
                    sent_tok = [sent_tok[i] for i in range(
                        len(sent_tok)) if tags[i][:1] in wanted]
                elif not nltk1:
                    tags = [word.pos_ for word in rich_word]
                    sent_tok = [sent_tok[i] for i in range(
                        len(sent_tok)) if tags[i] in wanted_scp]
            elif stemming:
                for idx in range(len(sent_tok)):
                    sent_tok[idx] = stemmer.stem(sent_tok[idx])

            if remove_stop:
                stop_words = stopwords.words('english')
                temp = []
                for tok in sent_tok:
                    if tok not in stop_words:
                        temp.append(tok)
                sent_tok = temp

            tokens.extend(sent_tok)
        result.append((tokens, 'padded'))
    return result
